{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">MIS 382N: ADVANCED PREDICTIVE MODELING - MSBA</p>\n",
    "# <p style=\"text-align: center;\">Assignment 1</p>\n",
    "## <p style=\"text-align: center;\">Total points: 100</p>\n",
    "## <p style=\"text-align: center;\">Due: Tuesday, September 17 submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Your partner needs to be from the same section. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTEID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group. If you do change, let the TA know. \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Challenges in Data Science (10 pts)\n",
    "\n",
    "Refer to the Domino guide under Modules --> Additional Resources\n",
    "\n",
    "Section 2 describes 8 Challenges. You may have personally encountered or heard of somebody else who encountered some of these challenge. If so,  please write 1-2 paragraphs on what situation was encountered and how it mapped into one the mentioned challenges. If not, think of a hypothetical case and do the same exercise. \n",
    "\n",
    "\n",
    "## Answer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During my summer internship at a startup in India, I encountered one of the problems mentioned in the white paper by Domino. The paper states it as : \"Solve the right problem, but didn't realise it had already been solved or can't reproduce it.\". I observed that the projects inside the company did not have the proper documentation for the code used. The only accessible data was the output of the whole projects in terms of insight or a metric that was tracked by a new dashboard. There was no way to access the thinking throughout the project which ###. I believe this is a drawback of chasing the problem in a competitive and timed environment. Data science being an analysis tool for the company and not the actual sellable product led to the company treating it for quick insights and directions to make decisions.\n",
    "\n",
    "I am sure there will be multiple times, when someone working on a different project can take help of already run projects and use part of the thinking and intermediate insights. One such case happened with me where I recreated the whole module to determine an attribute regarding the user of the product only to realise that this was already been done in some other project. Another problem that arises here is the absence of properly documented insights that might have appeared during the intermediate steps. The individualistic attention that a project gets tends to dictate the structure and collection formats of the data that might also lead to problems if the same data is used for some other kind of application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: MLE Estimate (5+10+10 points)\n",
    "\n",
    "Consider a coin tossing experiment where a biased coin is tossed repeatedly for $n$ times with independence in successive tosses. \n",
    "If we record the outcome of each toss as $X_{i}$, for $i \\in \\{1,2,3,....,n\\}$ as follows\n",
    "\n",
    "$  \n",
    "X_{i} = \n",
    "     \\begin{cases}\n",
    "       \\text{1,} &\\quad\\text{if $i^{th}$ toss results in $Heads$,}\\\\\n",
    "       \\text{0,} &\\quad\\text{otherwise.} \\\\ \n",
    "     \\end{cases}\n",
    "$\n",
    "\n",
    "then $X_{1}, X_{2}, .... X_{n}$ will be a sequence of $0$'s and $1$'s. Assume that for this coin $P(Heads) = p$, which of course is not known to the experimenter. \n",
    "\n",
    "1. The log-likelihood function of the observations, as discussed in the class, denotes the probability of occurrence of the observations. Write the log-likelihood function for the set of observations $X_{1}, X_{2}, .... X_{n}$. \n",
    "\n",
    "2. Compute an MLE estimate of $p$.\n",
    "\n",
    "3. Check if the obtained estimate is unbiased.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG-8994.jpg\"> <img src=\"IMG-8995.jpg\"> <img src=\"IMG-8996.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Multiple Linear Regression in Python (30 pts)\n",
    "\n",
    "Use the following code to import the California housing prices dataset and linear models in python. The dataset is taken from https://www.kaggle.com/camnugent/california-housing-prices/version/1. I have removed the categorical variables and rows with missing variables to make it easier to run the models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. (2 pts) Print the shape (number of rows and columns) of the feature matrix X, and print the first 5 rows.\n",
    "\n",
    "b.  (6 pts) Using ordinary least squares, fit a multiple linear regression (MLR) on all the feature variables using the entire dataset. Report the regression coefficient of each input feature and evaluate the model using mean absolute error (MAE).  Example of ordinary least squares in Python is shown in Section 1.1.1 of http://scikit-learn.org/stable/modules/linear_model.html.\n",
    "\n",
    "c.  (6 pts) Split the data into a training set and a test set, using the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with test_size = 0.30 and random_state = 11. Fit an MLR using the training set.  Evaluate the trained model using the training set and the test set, respectively.  Compare the two MAE values thus obtained.\n",
    "\n",
    "d.  (5 pts) Calculate the pearson correlation matrix of the independent variables in the training set (you can use [this](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html)). Report the variables which have magnitude of correlation greater than 0.9 w.r.t the variable 'households'. \n",
    "\n",
    "e.  (6 pts) Add the following independent variables to both train and test sets:\n",
    "1. average_bedrooms = total_bedrooms/households\n",
    "2. average_rooms = total_rooms/households\n",
    "3. average_population = total_rooms/households\n",
    "\n",
    "Recalculate the correlation matrix. What do you observe about the correlation values of the above new variables?\n",
    "\n",
    "f. (5 pts) Fit an MLR on the new train data (with additional independent variables) and report the MAE on the new train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent variables having correlation greater than 0.9 w.r.t 'households': \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Ridge and Lasso Regression (30 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same data from before, in this question you will explore the application of Lasso and Ridge regression using sklearn package in Python. Use the same train and test data with additional augmented columns from before. Scale the data so that each of the dependent variables have zero mean and unit variance. You can use the [sklearn.preprocessing.scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use sklearn.linear_model.Lasso and sklearn.linear_model.Ridge classes to do a [5-fold cross validation](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py) using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html). For the sweep of the regularization parameter, we will look at a grid of values ranging from $\\lambda = 10^{10}$ to $\\lambda = 10^{-2}$. In Python, you can consider this range of values as follows:\n",
    "\n",
    "      import numpy as np\n",
    "\n",
    "      alphas =  10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "  Report the best chosen $\\lambda$ based on cross validation. The cross validation should happen on your training data using  average MAE as the scoring metric. (8pts)\n",
    "\n",
    "2) Run ridge and lasso for all of the alphas specified above (on training data), and plot the coefficients learned for each of them - there should be one plot each for lasso and ridge, so a total of two plots; the plots for different features for a method should be on the same plot. What do you qualitatively observe when value of the regularization parameter is changed? (7pts)\n",
    "\n",
    "3) Run least squares regression, ridge, and lasso on the training data. For ridge and lasso, use only the best regularization parameter. Report the prediction error (MAE) on the test data for each. (5pts)\n",
    "\n",
    "4) Run lasso again with cross validation using [sklearn.linear_model.LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). Set the cross validation parameters as follows:\n",
    "\n",
    "    LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "\n",
    "Report the best $\\lambda$ based on cross validation. Run lasso on the training data using the best $\\lambda$ and report the coefficeints for all variables. (5pts)\n",
    "\n",
    "5) Why did we have to scale the data before regularization? (5pts)\n",
    "\n",
    "\n",
    "## Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting ridge coefficents as a function of the regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting lasso coefficents as a function of the regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the alpha parameter obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit lasso using the above alpha and report MAE on Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "When regularising linear regression either in the case of Ridge or in Lasso, a penalty is applied on the size of the cofficients. These methods are also called Shrinkage methods as the coefficient shrink towards zero and towards each other. In the absence of standardisation in the form of scaling, the shrinking will not be fair. There will be different contributions to the penalised terms from both independent variables having different scales. To avoid such kinds of problems, the independent variables are scaled before performing regularised regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (5pts)\n",
    "\n",
    "How do you think the performance of your model varies in the train and test set as you increase(decrease) the number of examples in the training dataset? Explain why does it change in a particular way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLAINATION: \n",
    "Let's approach the scenario in two different cases:\n",
    "1. Increase in the number of examples in the training dataset\n",
    "\n",
    "> More representative data: When the number of examples in the training dataset is increased the model's accuracy in the test dataset may increase as when there are more examples to learn from the model can improve itself to identify the trend.\n",
    "\n",
    "> Less chance of Overfitting: Increasing the number of examples can also mean that if the model was fitting to the noise in the data, it can be reduced. In other words: overfitting on the training data can reduce.\n",
    "\n",
    "2. Decrease in the number of examples in the training dataset\n",
    "\n",
    "> Chance of Overfitting: Decreasing the number of examples in the training dataset reduces the points on which the model is trained making the model more vulnerable to learn from the noise in data i.e. overfitting the data.\n",
    "\n",
    "> Unrepresentative data: Decreasing the number of examples might remove the data that depicts the trend. The model therefore might perform poor on the testing data set.\n",
    "\n",
    ">> The model's performance on the training set will always be satisfactory as it is the data on which it is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
